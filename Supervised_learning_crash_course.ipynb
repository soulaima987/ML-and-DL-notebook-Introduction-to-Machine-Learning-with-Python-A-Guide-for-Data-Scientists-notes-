{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["8ZP0b07xcPZi"],"authorship_tag":"ABX9TyMqPWud59qhWrtEVaz9j78m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Supervised Learning foundation :"],"metadata":{"id":"wJdq88jGNQNB"}},{"cell_type":"markdown","source":["Before we dive into the supervised learning algorithms we must fist know the data that we work with "],"metadata":{"id":"p0DvAzBzNbrN"}},{"cell_type":"markdown","source":["##Meet the Data\n"],"metadata":{"id":"bs_LXO8x7lmX"}},{"cell_type":"markdown","source":["There are several steps that can be taken to ensure that you meet the data correctly and get the most out of it:\n","-Understand the context of the data: Learn about the problem you're trying to solve, the industry and the data source. This will give you a sense of what type of data you're working with, and what kind of insights you can expect to gain from it.\n","\n","- Explore the data: Get a sense of the overall structure and characteristics of the data, including the number of observations, the types of variables, and their distribution.\n","\n","- Clean and preprocess the data: Address any missing or invalid data, handle outliers, and make sure the data is in a format that can be easily used for analysis.\n","\n","- Visualize the data: Use visualizations such as histograms, scatter plots, and box plots to gain insights into the distribution of the data and identify patterns or trends.\n","\n","- Feature engineering: Create new features or modify existing ones to better capture the information in the data that is relevant to the problem you're trying to solve.\n","\n","- Validate the data: Use techniques such as cross-validation to ensure that the data is reliable and that the model will generalize well to new data.\n","\n","- Document the data: Keep detailed notes on the data, including the source, cleaning and preprocessing steps, and any other relevant information."],"metadata":{"id":"IQJq280gnUZW"}},{"cell_type":"markdown","source":["Let's take an example using the iris datasets: "],"metadata":{"id":"KmjhwaMYn8u6"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"wh3KDZDCxFzy","executionInfo":{"status":"ok","timestamp":1674253892536,"user_tz":-60,"elapsed":1201,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"outputs":[],"source":["from sklearn.datasets import load_iris\n","iris_dataset=load_iris()"]},{"cell_type":"code","source":["print(\"keys of iris_dataset: \\n{}\".format(iris_dataset.keys()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xLn8tAEYaxC","executionInfo":{"status":"ok","timestamp":1674253892538,"user_tz":-60,"elapsed":85,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}},"outputId":"c38ebe41-ad15-4de8-a249-9ccf9ab181e7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["keys of iris_dataset: \n","dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"]}]},{"cell_type":"code","source":["print(iris_dataset['DESCR'][:193] + \"\\n...\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wRKFVV6gY-gV","executionInfo":{"status":"ok","timestamp":1674253892540,"user_tz":-60,"elapsed":81,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}},"outputId":"3b027dcb-4307-4143-9ae6-86ed98b50108"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":[".. _iris_dataset:\n","\n","Iris plants dataset\n","--------------------\n","\n","**Data Set Characteristics:**\n","\n","    :Number of Instances: 150 (50 in each of three classes)\n","    :Number of Attributes: 4 numeric, pre\n","...\n"]}]},{"cell_type":"code","source":["print(\"Target names: {}\".format(iris_dataset['target_names']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BK_TbV13aBcX","executionInfo":{"status":"ok","timestamp":1674253892542,"user_tz":-60,"elapsed":78,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}},"outputId":"e77514eb-f68d-4920-f933-afe1c45db089"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Target names: ['setosa' 'versicolor' 'virginica']\n"]}]},{"cell_type":"code","source":["print(\"Feature names: \\n{}\".format(iris_dataset['feature_names']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"STpaRb6zbpS2","executionInfo":{"status":"ok","timestamp":1674253892543,"user_tz":-60,"elapsed":71,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}},"outputId":"c064e3db-1e63-49f8-8264-74dd762ece63"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature names: \n","['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"]}]},{"cell_type":"code","source":["print(\"Type of data: {}\".format(type(iris_dataset['data'])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hBpHn6wucZGn","executionInfo":{"status":"ok","timestamp":1674253892544,"user_tz":-60,"elapsed":66,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}},"outputId":"d0a7d262-2091-4fd7-c220-e16e576c3233"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Type of data: <class 'numpy.ndarray'>\n"]}]},{"cell_type":"code","source":["iris_dataset['data'].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wamoOagrdQkh","executionInfo":{"status":"ok","timestamp":1674253892546,"user_tz":-60,"elapsed":61,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}},"outputId":"e51c5a88-d8cf-45d9-f42b-66b4a3ba1e30"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(150, 4)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test=train_test_split(iris_dataset['data'],iris_dataset['target'],random_state=0)"],"metadata":{"id":"guUHfRlcddm0","executionInfo":{"status":"ok","timestamp":1674253892546,"user_tz":-60,"elapsed":56,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["print(\"X_train shape: {}\".format(X_train.shape))\n","print(\"y_train shape: {}\".format(y_train.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4M1IHTVWnivL","executionInfo":{"status":"ok","timestamp":1674253892547,"user_tz":-60,"elapsed":56,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}},"outputId":"5aed7554-1db0-4cbe-9c14-e191f0ca24b5"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape: (112, 4)\n","y_train shape: (112,)\n"]}]},{"cell_type":"code","source":["print(\"X_test shape: {}\".format(X_test.shape))\n","print(\"y_test shape: {}\".format(y_test.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lvnCrYlHo2Ll","executionInfo":{"status":"ok","timestamp":1674253892548,"user_tz":-60,"elapsed":53,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}},"outputId":"da1f34c2-7cf4-4567-fbc8-8a99d2dd5342"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["X_test shape: (38, 4)\n","y_test shape: (38,)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","df=pd.DataFrame(iris_dataset[\"data\"])"],"metadata":{"id":"y7Zcxq-Yo5eU","executionInfo":{"status":"ok","timestamp":1674253892549,"user_tz":-60,"elapsed":49,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["###First Things First: Look at Your Data"],"metadata":{"id":"R9ui2fCX76ru"}},{"cell_type":"code","source":["iris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)"],"metadata":{"id":"6dYJ59PRr3nc","executionInfo":{"status":"ok","timestamp":1674253892550,"user_tz":-60,"elapsed":48,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["iris_dataframe"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"VTo420pfuRdj","executionInfo":{"status":"ok","timestamp":1674253892551,"user_tz":-60,"elapsed":48,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}},"outputId":"d51f0c18-f0a9-46cb-c94d-c23cf2c7afcd"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n","0                  5.9               3.0                4.2               1.5\n","1                  5.8               2.6                4.0               1.2\n","2                  6.8               3.0                5.5               2.1\n","3                  4.7               3.2                1.3               0.2\n","4                  6.9               3.1                5.1               2.3\n","..                 ...               ...                ...               ...\n","107                4.9               3.1                1.5               0.1\n","108                6.3               2.9                5.6               1.8\n","109                5.8               2.7                4.1               1.0\n","110                7.7               3.8                6.7               2.2\n","111                4.6               3.2                1.4               0.2\n","\n","[112 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-aa22d626-8661-40a9-b39f-71f47169f5ce\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sepal length (cm)</th>\n","      <th>sepal width (cm)</th>\n","      <th>petal length (cm)</th>\n","      <th>petal width (cm)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5.9</td>\n","      <td>3.0</td>\n","      <td>4.2</td>\n","      <td>1.5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5.8</td>\n","      <td>2.6</td>\n","      <td>4.0</td>\n","      <td>1.2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6.8</td>\n","      <td>3.0</td>\n","      <td>5.5</td>\n","      <td>2.1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4.7</td>\n","      <td>3.2</td>\n","      <td>1.3</td>\n","      <td>0.2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6.9</td>\n","      <td>3.1</td>\n","      <td>5.1</td>\n","      <td>2.3</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>107</th>\n","      <td>4.9</td>\n","      <td>3.1</td>\n","      <td>1.5</td>\n","      <td>0.1</td>\n","    </tr>\n","    <tr>\n","      <th>108</th>\n","      <td>6.3</td>\n","      <td>2.9</td>\n","      <td>5.6</td>\n","      <td>1.8</td>\n","    </tr>\n","    <tr>\n","      <th>109</th>\n","      <td>5.8</td>\n","      <td>2.7</td>\n","      <td>4.1</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>110</th>\n","      <td>7.7</td>\n","      <td>3.8</td>\n","      <td>6.7</td>\n","      <td>2.2</td>\n","    </tr>\n","    <tr>\n","      <th>111</th>\n","      <td>4.6</td>\n","      <td>3.2</td>\n","      <td>1.4</td>\n","      <td>0.2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>112 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aa22d626-8661-40a9-b39f-71f47169f5ce')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-aa22d626-8661-40a9-b39f-71f47169f5ce button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-aa22d626-8661-40a9-b39f-71f47169f5ce');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["pip install mglearn"],"metadata":{"id":"yg7LpQN6zt8v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1674253900330,"user_tz":-60,"elapsed":7818,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}},"outputId":"0b8c2af2-9522-4a0b-a0ab-1850b022f9de"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mglearn\n","  Downloading mglearn-0.1.9.tar.gz (540 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 KB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mglearn) (1.21.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mglearn) (2.0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from mglearn) (0.22)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from mglearn) (1.3.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from mglearn) (7.1.2)\n","Requirement already satisfied: cycler in /usr/local/lib/python3.8/dist-packages (from mglearn) (0.11.0)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from mglearn) (2.9.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from mglearn) (1.2.0)\n","Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mglearn) (1.15.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mglearn) (3.0.9)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from matplotlib->mglearn) (2022.7)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from matplotlib->mglearn) (2.8.2)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->mglearn) (1.7.3)\n","Building wheels for collected packages: mglearn\n","  Building wheel for mglearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mglearn: filename=mglearn-0.1.9-py2.py3-none-any.whl size=582637 sha256=100940db604f229a9aee48f00b206971ad7f4a0fef8d8481b309580724639544\n","  Stored in directory: /root/.cache/pip/wheels/87/75/37/404e66d0c4bad150f101c9a0914b11a8eccc2681559936e7f7\n","Successfully built mglearn\n","Installing collected packages: mglearn\n","Successfully installed mglearn-0.1.9\n"]}]},{"cell_type":"markdown","source":["NOTE :  mglearn is a library that is build on top of scikit-learn, which is another machine learning library."],"metadata":{"id":"gD12zriXq9mL"}},{"cell_type":"code","source":["import mglearn"],"metadata":{"id":"CslkcRX80YFo","executionInfo":{"status":"error","timestamp":1674253900919,"user_tz":-60,"elapsed":597,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}},"colab":{"base_uri":"https://localhost:8080/","height":311},"outputId":"cd547d9c-2d53-424f-dd67-86467a15c081"},"execution_count":20,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-26a0454c3f6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmglearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/mglearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcm3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdiscrete_scatter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplot_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReBl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/mglearn/plots.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplot_tree_nonmonotonous\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_tree_not_monotone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplot_scaling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplot_pca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_pca_illustration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_pca_whitening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_pca_faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplot_decomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mplot_nmf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_nmf_illustration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_nmf_faces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/mglearn/plot_pca.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjoblib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcachedir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cache\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'cachedir'"]}]},{"cell_type":"markdown","source":["- In order to use mglearn in Google Colab, you'll need to first install it. You can do this by running !pip install mglearn in a code cell.\n","- It's important to note that the library may not be compatible with the current version of the other libraries in your environment. So, you may need to check the version of your libraries and match them with the compatible version of mglearn library."],"metadata":{"id":"m21KTOZYroR0"}},{"cell_type":"code","source":["grr = pd.plotting.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o',\n"," hist_kwds={'bins': 20}, s=60, alpha=.8, cmap=mglearn.cm3)"],"metadata":{"id":"fIVaxpNguUxm","executionInfo":{"status":"aborted","timestamp":1674253900921,"user_tz":-60,"elapsed":23,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Building Your First Model: k-Nearest Neighbors"],"metadata":{"id":"OFUCgv0u7_Pl"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","knn=KNeighborsClassifier(n_neighbors=1)"],"metadata":{"id":"HGfE2BzKu5Ia","executionInfo":{"status":"aborted","timestamp":1674253900922,"user_tz":-60,"elapsed":24,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can start building the actual machine learning model. There are many classi‐\n","fication algorithms in scikit-learn that we could use. Here we will use a k-nearest\n","neighbors classifier, which is easy to understand. Building this model only consists of\n","storing the training set. To make a prediction for a new data point, the algorithm\n","finds the point in the training set that is closest to the new point. Then it assigns the\n","label of this training point to the new data point.\n"],"metadata":{"id":"VILvk2BT8Dk-"}},{"cell_type":"code","source":["knn.fit(X_train,y_train)"],"metadata":{"id":"-Mp3mLGa8DKu","executionInfo":{"status":"aborted","timestamp":1674253900923,"user_tz":-60,"elapsed":25,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Making a predictions"],"metadata":{"id":"WUo7FD9KYky3"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"rjY9SVhcZoiC","executionInfo":{"status":"aborted","timestamp":1674253900923,"user_tz":-60,"elapsed":25,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_new=np.array([[5,2.9,1,0.2]])\n","print(\"X_new.shape:{}\".format(X_new.shape))"],"metadata":{"id":"Z315ndWLX7fA","executionInfo":{"status":"aborted","timestamp":1674253900924,"user_tz":-60,"elapsed":25,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction=knn.predict(X_new)\n","iris_dataset['target_names'][prediction]"],"metadata":{"id":"Y1tLrgR9ZmL8","executionInfo":{"status":"aborted","timestamp":1674253900925,"user_tz":-60,"elapsed":26,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Evaluating the Model"],"metadata":{"id":"8ZP0b07xcPZi"}},{"cell_type":"markdown","source":["This is where the test set that we created earlier comes in. This data was not used to\n","build the model, but we do know what the correct species is for each iris in the test\n","set.\n","Therefore, we can make a prediction for each iris in the test data and compare it\n","against its label (the known species). We can measure how well the model works by\n","computing the accuracy, which is the fraction of flowers for which the right species\n","was predicted:"],"metadata":{"id":"szf44IV9cfmU"}},{"cell_type":"code","source":["y_pred=knn.predict(X_test)\n","y_pred"],"metadata":{"id":"muXCI0Xtal5m","executionInfo":{"status":"aborted","timestamp":1674253900925,"user_tz":-60,"elapsed":26,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.mean(y_pred==y_test)"],"metadata":{"id":"4OXnWrqwcEne","executionInfo":{"status":"aborted","timestamp":1674253900926,"user_tz":-60,"elapsed":26,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["knn.score(X_test,y_test)"],"metadata":{"id":"-uSO7nCXdJPS","executionInfo":{"status":"aborted","timestamp":1674253900926,"user_tz":-60,"elapsed":26,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Chapter 2"],"metadata":{"id":"9LhElbmme22q"}},{"cell_type":"markdown","source":["##Classification and Regression"],"metadata":{"id":"OkVIN9w1e6ZR"}},{"cell_type":"markdown","source":["### Overfitting and underfitting :"],"metadata":{"id":"4xXkl87b6d_x"}},{"cell_type":"markdown","source":["Overfitting\n","occurs when you fit a model too closely to the particularities of the training set and\n","obtain a model that works well on the training set but is not able to generalize to new\n","data. On the other hand, if your model is too simple say, “Everybody who owns a\n","house buys a boat” then you might not be able to capture all the aspects of and variability in the data, and your model will do badly even on the training set. that's called underfitting"],"metadata":{"id":"E-ZNIV0RhIM7"}},{"cell_type":"markdown","source":["The more complex we allow our model to be, the better we will be able to predict on\n","the training data. However, if our model becomes too complex, we start focusing too\n","much on each individual data point in our training set, and the model will not gener‐\n","alize well to new data.\n","There is a sweet spot in between that will yield the best generalization performance.\n","This is the model we want to find.\n"],"metadata":{"id":"l1gWJYBihhy3"}},{"cell_type":"markdown","source":["### Classification VS Regression\n","- In regression, the input data is often represented by a set of features or attributes, such as the size and age of a house, and the output is a continuous variable, such as the price of the house. The process of regression is to learn a mapping from the input features to the output variable, based on the labeled examples in the training data.\n","- Classification is a supervised learning problem in which the goal is to predict a categorical label or class from a set of input features. It is a type of machine learning problem in which an algorithm learns from a labeled dataset, and then uses this knowledge to predict the class or label"],"metadata":{"id":"Tl8IKSVC6ll_"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","# generate dataset\n","X, y = mglearn.datasets.make_forge()\n","# plot dataset\n","mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n","plt.legend([\"Class 0\", \"Class 1\"], loc=4)\n","plt.xlabel(\"First feature\")\n","plt.ylabel(\"Second feature\")\n","print(\"X.shape: {}\".format(X.shape))"],"metadata":{"id":"EHs-Mpn5eWfV","executionInfo":{"status":"aborted","timestamp":1674253900927,"user_tz":-60,"elapsed":27,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","cancer=load_breast_cancer()\n","cancer.keys()"],"metadata":{"id":"4Mg7aEmkk9Ip","executionInfo":{"status":"aborted","timestamp":1674253900927,"user_tz":-60,"elapsed":26,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df=pd.DataFrame(cancer['data'])\n","df"],"metadata":{"id":"QlBovLhqxmjv","executionInfo":{"status":"aborted","timestamp":1674253900928,"user_tz":-60,"elapsed":27,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.info()"],"metadata":{"id":"KNEMpudlxwkQ","executionInfo":{"status":"aborted","timestamp":1674253900929,"user_tz":-60,"elapsed":28,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Sample counts per class:\\n{}\".format(\n"," {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))"],"metadata":{"id":"ZPFYRuoZx3Lo","executionInfo":{"status":"aborted","timestamp":1674253900929,"user_tz":-60,"elapsed":27,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Sample counts per class:\\n{}\".format(\n"," {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))"],"metadata":{"id":"hevy8tk725Xo","executionInfo":{"status":"aborted","timestamp":1674253900930,"user_tz":-60,"elapsed":28,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cancer.feature_names"],"metadata":{"id":"MGbAx9em4Yr1","executionInfo":{"status":"aborted","timestamp":1674253900931,"user_tz":-60,"elapsed":29,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(cancer.DESCR)"],"metadata":{"id":"sOswsl3p44Jp","executionInfo":{"status":"aborted","timestamp":1674253900931,"user_tz":-60,"elapsed":28,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###analyse bosten housing"],"metadata":{"id":"y7pTe42W5p7W"}},{"cell_type":"markdown","source":["building a model follow this steps:\n","loading or importing the data -> spliting the data into test and valisation set ->choose the algorithms to train and fit the data -> predict -> score the model"],"metadata":{"id":"joyZBU1BD9y_"}},{"cell_type":"code","source":["from mglearn.datasets import load_extended_boston\n","boston=load_extended_boston"],"metadata":{"id":"yPyJerSX6FjI","executionInfo":{"status":"aborted","timestamp":1674253900932,"user_tz":-60,"elapsed":29,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X,y=mglearn.datasets.load_extended_boston()"],"metadata":{"id":"PzCQKmQV8UDn","executionInfo":{"status":"aborted","timestamp":1674253900932,"user_tz":-60,"elapsed":20764,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mglearn.plots.plot_knn_classification(n_neighbors=1)"],"metadata":{"id":"ErRPP1K89utu","executionInfo":{"status":"aborted","timestamp":1674253900933,"user_tz":-60,"elapsed":20764,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mglearn.plots.plot_knn_classification(n_neighbors=3)"],"metadata":{"id":"VdscYLLKJ22q","executionInfo":{"status":"aborted","timestamp":1674253900934,"user_tz":-60,"elapsed":20763,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","X,y=mglearn.datasets.make_forge()\n","X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)"],"metadata":{"id":"8G6yw57_Lfei","executionInfo":{"status":"aborted","timestamp":1674253900935,"user_tz":-60,"elapsed":20763,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","from sklearn.neighbors import KNeighborsClassifier\n","clf=KNeighborsClassifier(n_neighbors=3)"],"metadata":{"id":"oQClTWpkMUMa","executionInfo":{"status":"aborted","timestamp":1674253900937,"user_tz":-60,"elapsed":20764,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf.fit(X_train,y_train)"],"metadata":{"id":"WEmOmrmBMwdj","executionInfo":{"status":"aborted","timestamp":1674253900938,"user_tz":-60,"elapsed":20764,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf.predict(X_test)"],"metadata":{"id":"Y4P3limSM3-b","executionInfo":{"status":"aborted","timestamp":1674253900939,"user_tz":-60,"elapsed":20764,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf.score(X_test,y_test)"],"metadata":{"id":"N8Hjoj3PNhpx","executionInfo":{"status":"aborted","timestamp":1674253900940,"user_tz":-60,"elapsed":20764,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axes=plt.subplots(1,3,figsize=(10,3))\n","for n_neighbors,ax in zip([1,3,9],axes):\n","  clf=KNeighborsClassifier(n_neighbors=n_neighbors).fit(X,y)\n","  mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n","  mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n","  ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n","  ax.set_xlabel(\"feature 0\")\n","  ax.set_ylabel(\"feature 1\")\n","axes[0].legend(loc=3)\n"],"metadata":{"id":"hf9tNqpmNv0P","executionInfo":{"status":"aborted","timestamp":1674253901364,"user_tz":-60,"elapsed":11,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["compairing the models using 1,3 and 9 neighbors"],"metadata":{"id":"5iDAp6JzFrGZ"}},{"cell_type":"markdown","source":["###Using the breast cancer dataset"],"metadata":{"id":"BGIg9PEJEzfs"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","cancer=load_breast_cancer()\n","X_train,X_test,y_train,y_test=train_test_split(cancer.data,cancer.target,stratify=cancer.target,random_state=66)\n","training_accuracy = []\n","test_accuracy = []\n","# try n_neighbors from 1 to 10\n","neighbors_settings = range(1, 11)\n","for n_neighbors in neighbors_settings:\n"," # build the model\n"," clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n"," clf.fit(X_train, y_train)\n"," # record training set accuracy\n"," training_accuracy.append(clf.score(X_train, y_train))\n"," # record generalization accuracy\n"," test_accuracy.append(clf.score(X_test, y_test))\n","plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n","plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n","plt.ylabel(\"Accuracy\")\n","plt.xlabel(\"n_neighbors\")\n","plt.legend()\n"],"metadata":{"id":"lG3K1VaXOlPk","executionInfo":{"status":"aborted","timestamp":1674253901365,"user_tz":-60,"elapsed":12,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The best performance is somewhere in the middle,\n","using around six neighbors. Still, it is good to keep the scale of the plot in mind. The\n","worst performance is around 88% accuracy, which might still be acceptable.\n"],"metadata":{"id":"wq2yQpinSLri"}},{"cell_type":"code","source":["mglearn.plots.plot_knn_regression(n_neighbors=1)"],"metadata":{"id":"08bfuN7SQ4e5","executionInfo":{"status":"aborted","timestamp":1674253901365,"user_tz":-60,"elapsed":12,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsRegressor\n","X,y=mglearn.datasets.make_wave(n_samples=40)\n","# split the wave dataset into a training and a test set\n","X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n","# instantiate the model and set the number of neighbors to consider to 3\n","reg=KNeighborsRegressor(n_neighbors=3)\n","reg.fit(X_train,y_train)"],"metadata":{"id":"0AsG4HjdTNUC","executionInfo":{"status":"aborted","timestamp":1674253901365,"user_tz":-60,"elapsed":12,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reg.predict(X_test)"],"metadata":{"id":"yGx50JoDUHNM","executionInfo":{"status":"aborted","timestamp":1674253901366,"user_tz":-60,"elapsed":13,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reg.score(X_test,y_test)"],"metadata":{"id":"qMnk3X6nXlgp","executionInfo":{"status":"aborted","timestamp":1674253901366,"user_tz":-60,"elapsed":12,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Analyzing KNeighborsRegressor"],"metadata":{"id":"d_5BryXJTX9q"}},{"cell_type":"code","source":["fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n","# create 1,000 data points, evenly spaced between -3 and 3\n","line = np.linspace(-3, 3, 1000).reshape(-1, 1)\n","for n_neighbors, ax in zip([1, 3, 9], axes):\n"," # make predictions using 1, 3, or 9 neighbors\n"," reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n"," reg.fit(X_train, y_train)\n"," ax.plot(line, reg.predict(line))\n"," ax.plot(X_train, y_train, '^', c=mglearn.cm2(0), markersize=8)\n"," ax.plot(X_test, y_test, 'v', c=mglearn.cm2(1), markersize=8)\n"," ax.set_title(\n"," \"{} neighbor(s)\\n train score: {:.2f} test score: {:.2f}\".format(\n"," n_neighbors, reg.score(X_train, y_train),\n"," reg.score(X_test, y_test)))\n"," ax.set_xlabel(\"Feature\")\n"," ax.set_ylabel(\"Target\")\n","axes[0].legend([\"Model predictions\", \"Training data/target\",\n"," \"Test data/target\"], loc=\"best\")\n"],"metadata":{"id":"FR2cGEbjcSE0","executionInfo":{"status":"aborted","timestamp":1674253901366,"user_tz":-60,"elapsed":12,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["So, while the nearest k-neighbors algorithm is easy to understand, it is not often used\n","in practice, due to prediction being slow and its inability to handle many features.\n","The method we discuss next has neither of these drawbacks.\n"],"metadata":{"id":"Jiu13Mj8lRTX"}},{"cell_type":"markdown","source":["One of the strengths of k-NN is that the model is very easy to understand, and often\n","gives reasonable performance without a lot of adjustments. Using this algorithm is a\n","good baseline method to try before considering more advanced techniques. Building\n","the nearest neighbors model is usually very fast, but when your training set is very\n","large"],"metadata":{"id":"pnFHOIi6lW5c"}},{"cell_type":"markdown","source":["#Linear Models\n"],"metadata":{"id":"-snf1bIIlbKl"}},{"cell_type":"markdown","source":["##Linear models for regression\n"],"metadata":{"id":"ru4BQsXzl3Dx"}},{"cell_type":"code","source":["mglearn.plots.plot_linear_regression_wave()"],"metadata":{"id":"4C5HoAEpjV2t","executionInfo":{"status":"aborted","timestamp":1674253901366,"user_tz":-60,"elapsed":12,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["There are many different linear models for regression. The difference between these\n","models lies in how the model parameters w and b are learned from the training data"],"metadata":{"id":"40yUNjB0qh07"}},{"cell_type":"markdown","source":["##Linear regression (ordinary squares)"],"metadata":{"id":"5ebtldOrq11T"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","X,y=mglearn.datasets.make_wave(n_samples=60)\n","X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42)\n","lr=LinearRegression()\n","lr.fit(X_train,y_train)"],"metadata":{"id":"XlXvRr9GojpM","executionInfo":{"status":"aborted","timestamp":1674253901367,"user_tz":-60,"elapsed":13,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))"],"metadata":{"id":"pNASetjEVMhR","executionInfo":{"status":"aborted","timestamp":1674253901367,"user_tz":-60,"elapsed":13,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["An R\n","2\n"," of around 0.66 is not very good, but we can see that the scores on the training\n","and test sets are very close together. This means we are likely underfitting, not over‐\n","fitting. For this one-dimensional dataset, there is little danger of overfitting, as the\n","model is very simple"],"metadata":{"id":"oRPlmBtOYOa0"}},{"cell_type":"code","source":["X,y=mglearn.datasets.load_extended_boston()\n","X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)\n","lr=LinearRegression().fit(X_train,y_train)"],"metadata":{"id":"twciPqkPXkkw","executionInfo":{"status":"aborted","timestamp":1674253901367,"user_tz":-60,"elapsed":13,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n"],"metadata":{"id":"ldny_KdZY2Jf","executionInfo":{"status":"aborted","timestamp":1674253901367,"user_tz":-60,"elapsed":12,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This discrepancy between performance on the training set and the test set is a clear\n","sign of overfitting, and therefore we should try to find a model that allows us to control complexity. One of the most commonly used alternatives to standard linear\n","regression is ridge regression, which we will look into next."],"metadata":{"id":"8MLNloZTZ8Hi"}},{"cell_type":"markdown","source":["###Ridge regression"],"metadata":{"id":"VYYo78V4im2_"}},{"cell_type":"code","source":["from sklearn.linear_model import Ridge\n","ridge=Ridge().fit(X_train,y_train)\n","print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))"],"metadata":{"id":"fF6HNkC_ZR3A","executionInfo":{"status":"aborted","timestamp":1674253901368,"user_tz":-60,"elapsed":13,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[". A less complex model means worse performance on the training\n","set, but better generalization. As we are only interested in generalization perfor‐\n","mance, we should choose the Ridge model over the LinearRegression model."],"metadata":{"id":"dc3RzAoFgh3V"}},{"cell_type":"markdown","source":["The optimum setting of alpha depends on the particular dataset we are using.\n","Increasing alpha forces coefficients to move more toward zero, which decreases\n","training set performance but might help generalization.\n"],"metadata":{"id":"8970FhONjVHM"}},{"cell_type":"markdown","source":["The lesson here is that with enough training data, regularization becomes less important, and given enough data, ridge and linear regression will have the same performance."],"metadata":{"id":"BMwY_uh3sCdc"}},{"cell_type":"markdown","source":["###Lasso"],"metadata":{"id":"6-NTpW4csaHG"}},{"cell_type":"markdown","source":["called also L1 regularization\n"," The consequence of L1 regularization\n","is that when using the lasso, some coefficients are exactly zero. This means some features are entirely ignored by the model. This can be seen as a form of automatic feature selection. Having some coefficients be exactly zero often makes a model easier to\n","interpret, and can reveal the most important features of your model."],"metadata":{"id":"jHDhrUgZsjzJ"}},{"cell_type":"code","source":["from sklearn.linear_model import Lasso\n","lasso=Lasso().fit(X_train,y_train)"],"metadata":{"id":"5McwpKcVsk5q","executionInfo":{"status":"aborted","timestamp":1674253901368,"user_tz":-60,"elapsed":13,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n","print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))"],"metadata":{"id":"coH1Aclbs4x5","executionInfo":{"status":"aborted","timestamp":1674253901368,"user_tz":-60,"elapsed":13,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\n","print(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\n","print(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))"],"metadata":{"id":"RsK-zaKz0tKj","executionInfo":{"status":"aborted","timestamp":1674253901368,"user_tz":-60,"elapsed":13,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we set alpha too low, however, we again remove the effect of regularization and end\n","up overfitting, with a result similar to LinearRegression:"],"metadata":{"id":"s4Denhjy0-dG"}},{"cell_type":"code","source":["lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\n","print(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\n","print(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\n","print(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))"],"metadata":{"id":"IjHu7Gj-0xEO","executionInfo":{"status":"aborted","timestamp":1674253901368,"user_tz":-60,"elapsed":13,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Ridge regression is usually the first choice between these two models.\n","- Lasso might be a better choice ridge regression is usually the first choice between these two models.if you would like to have a\n","model that is easy to interpret, Lasso will provide a model that is easier to understand, as it will select only a subset of the input features.\n","\n","- scikit-learn also provides\n","the ElasticNet class, which combines the penalties of Lasso and Ridge"],"metadata":{"id":"YAbo1tY-5W4B"}},{"cell_type":"markdown","source":["##Linear models for classification"],"metadata":{"id":"8p8wfN6T9SXO"}},{"cell_type":"markdown","source":["Linear models are also extensively used for classification. Let’s look at binary classifi‐\n","cation first. In this case, a prediction is made using the following formula:\n","\n","ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\n","\n","The formula looks very similar to the one for linear regression, but instead of just\n","returning the weighted sum of the features, we threshold the predicted value at zero.\n","If the function is smaller than zero, we predict the class –1; if it is larger than zero, we\n","predict the class +1. This prediction rule is common to all linear models for classifica‐\n","tion. Again, there are many different ways to find the coefficients (w) and the intercept (b)."],"metadata":{"id":"4ACdtwlS9tfd"}},{"cell_type":"markdown","source":["There are many algorithms for learning linear models. These algorithms all differ in the following two ways:\n","\n","- The way in which they measure how well a particular combination of coefficients\n","\n","and intercept fits the training data\n","- If and what kind of regularization they use"],"metadata":{"id":"MXhl_bSx_WXJ"}},{"cell_type":"code","source":["#there are two most common linear classification algorithms are:\n","#logistic regression\n","#linear support vector machine\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import LinearSVC\n","X, y = mglearn.datasets.make_forge()\n","fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n","for model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n"," clf = model.fit(X, y)\n"," mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n"," ax=ax, alpha=.7)\n"," mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n"," ax.set_title(\"{}\".format(clf.__class__.__name__))\n"," ax.set_xlabel(\"Feature 0\")\n"," ax.set_ylabel(\"Feature 1\")\n","axes[0].legend()\n"],"metadata":{"id":"uFSula3p1FCx","executionInfo":{"status":"aborted","timestamp":1674253901369,"user_tz":-60,"elapsed":14,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mglearn.plots.plot_linear_svc_regularization()\n"],"metadata":{"id":"izOaaC2zF-DG","executionInfo":{"status":"aborted","timestamp":1674253901369,"user_tz":-60,"elapsed":14,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["we have a very small C corresponding to a lot of regularization. The strongly regularized model chooses a relatively horizontal line, misclassifying two points\n","\n","In the center plot, C is slightly higher, and the model focuses more\n","on the two misclassified samples, tilting the decision boundary\n","\n"],"metadata":{"id":"LIDyS0NtGJEg"}},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.linear_model import LogisticRegression\n","cancer=load_breast_cancer()\n","X_train,X_test,y_train,y_test=train_test_split(cancer.data,cancer.target,random_state=42)\n","logreg=LogisticRegression().fit(X_train,y_train)\n","print(logreg.score(X_train,y_train))\n","print(logreg.score(X_test,y_test))"],"metadata":{"id":"Rf9XjelJGDt_","executionInfo":{"status":"aborted","timestamp":1674253901369,"user_tz":-60,"elapsed":14,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The default value of C=1 provides quite good performance, with 95% accuracy on\n","both the training and the test set. But as training and test set performance are very\n","close, it is likely that we are underfitting. Let’s try to increase C to fit a more flexible\n","model:\n"],"metadata":{"id":"RA5A5OViWd3g"}},{"cell_type":"code","source":["logreg100=LogisticRegression(C=100).fit(X_train,y_train)\n","print(logreg100.score(X_train,y_train))\n","print(logreg100.score(X_test,y_test))"],"metadata":{"id":"QRC0JNVNKVsQ","executionInfo":{"status":"aborted","timestamp":1674253901369,"user_tz":-60,"elapsed":14,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logreg001=LogisticRegression(C=0.01).fit(X_train,y_train)\n","print(logreg001.score(X_train,y_train))\n","print(logreg001.score(X_test,y_test))"],"metadata":{"id":"LkcIb44IWyeR","executionInfo":{"status":"aborted","timestamp":1674253901370,"user_tz":-60,"elapsed":15,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Linear Model for multiclass classification"],"metadata":{"id":"EXab_t7rgTd7"}},{"cell_type":"code","source":["from sklearn import linear_model\n","from sklearn.datasets import make_blobs\n","X,y=make_blobs(random_state=42)\n","mglearn.discrete_scatter(X[:,0],X[:,1],y)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")\n","plt.legend([\"class 0\",\"Class 1\",\"class 2\"])\n","linear_svm=LinearSVC().fit(X,y)\n","print(linear_svm.coef_.shape)\n","print(linear_svm.intercept_.shape)\n"],"metadata":{"id":"S9d0RDlSYQ2x","executionInfo":{"status":"aborted","timestamp":1674253901370,"user_tz":-60,"elapsed":15,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Decision Trees"],"metadata":{"id":"eha2g82PYnVH"}},{"cell_type":"markdown","source":["the decision trees try to find the perfect split of region that is the most informative oneabout the target variable with the recursuve process.\n","\n","The recursive partitioning of the data is repeated until each region in the partition\n","(each leaf in the decision tree) only contains a single target value (a single class or a\n","single regression value). A leaf of the tree that contains data points that all share the\n","same target value is called pure"],"metadata":{"id":"_o-AEFtg8cXp"}},{"cell_type":"markdown","source":["A prediction on a new data point is made by checking which region of the partition\n","of the feature space the point lies in, and then predicting the majority target (or the\n","single target in the case of pure leaves) in that region. The region can be found by\n","traversing the tree from the root and going left or right, depending on whether the\n","test is fulfilled or not"],"metadata":{"id":"xT71JKBf9nPv"}},{"cell_type":"code","source":["mglearn.plots.plot_animal_tree()"],"metadata":{"id":"iZpkZz9PiUzL","executionInfo":{"status":"aborted","timestamp":1674253901370,"user_tz":-60,"elapsed":15,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Controlling complexity of decision trees\n","There are two common strategies to prevent overfitting: stopping the creation of the\n","tree early (also called pre-pruning), or building the tree but then removing or collaps‐\n","ing nodes that contain little information (also called post-pruning or just pruning).\n","Possible criteria for pre-pruning include limiting the maximum depth of the tree,\n","limiting the maximum number of leaves, or requiring a minimum number of points\n","in a node to keep splitting it."],"metadata":{"id":"TTbN4wWZ-gMp"}},{"cell_type":"markdown","source":["###Building decision trees"],"metadata":{"id":"tGHC20W9ZOcW"}},{"cell_type":"markdown","source":["A leaf of the tree that contains data points that all share the\n","same target value is called pure."],"metadata":{"id":"LtKQA-VFbCfe"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","cancer=load_breast_cancer()\n","X_train,X_test,y_train,y_test=train_test_split(cancer.data,cancer.target,random_state=0)\n","tree=DecisionTreeClassifier()\n","tree.fit(X_train,y_train)\n","print(tree.score(X_train,y_train))\n","print(tree.score(X_test,y_test))"],"metadata":{"id":"bP5Fdv7XYvxS","executionInfo":{"status":"aborted","timestamp":1674253901370,"user_tz":-60,"elapsed":15,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we don’t restrict the depth of a decision tree, the tree can become arbitrarily deep\n","and complex. Unpruned trees are therefore prone to overfitting and not generalizing\n","well to new data. Now let’s apply pre-pruning to the tree, which will stop developing\n","the tree before we perfectly fit to the training data. One option is to stop building the\n","tree after a certain depth has been reached. Here we set max_depth=4, meaning only\n","four consecutive questions can be asked. Limiting the\n","depth of the tree decreases overfitting. This leads to a lower accuracy on the training\n","set, but an improvement on the test set:"],"metadata":{"id":"t35jnIIpd_0x"}},{"cell_type":"code","source":["tree=DecisionTreeClassifier(max_depth=4,random_state=0)\n","tree.fit(X_train,y_train)\n","print(tree.score(X_train,y_train))\n","print(tree.score(X_test,y_test))"],"metadata":{"id":"wp3FglAzddLW","executionInfo":{"status":"aborted","timestamp":1674253901371,"user_tz":-60,"elapsed":15,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Analysing the decision tree"],"metadata":{"id":"7ZJkGDceeidu"}},{"cell_type":"code","source":["from sklearn.tree import export_graphviz\n","export_graphviz(tree,out_file='tree_dot',class_names=[\"malignant\",\"benign\"],feature_names=cancer.feature_names,impurity=False,filled=True)"],"metadata":{"id":"04ZQQIpvefwH","executionInfo":{"status":"aborted","timestamp":1674253901371,"user_tz":-60,"elapsed":15,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.tree import export_graphviz\n","export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\n"," feature_names=cancer.feature_names, impurity=False, filled=True)"],"metadata":{"id":"0__mumphgNk0","executionInfo":{"status":"aborted","timestamp":1674253901371,"user_tz":-60,"elapsed":15,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import graphviz\n","with open(\"tree.dot\") as f:\n","  dot_graph=f.read()\n","  graphviz.Source(dot_graph)"],"metadata":{"id":"VOyojTJ3fKnK","executionInfo":{"status":"aborted","timestamp":1674253901371,"user_tz":-60,"elapsed":15,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###features importance in trees"],"metadata":{"id":"3_hzLnAXhRGf"}},{"cell_type":"markdown","source":["The most commonly\n","used summary is feature importance, which rates how important each feature is for\n","the decision a tree makes. It is a number between 0 and 1 for each feature, where 0\n","means “not used at all” and 1 means “perfectly predicts the target.” The feature\n","importances always sum to 1:\n"],"metadata":{"id":"FaKK6MXIhl7Z"}},{"cell_type":"code","source":["print(tree.feature_importances_)"],"metadata":{"id":"QSayH4uNfxc7","executionInfo":{"status":"aborted","timestamp":1674253901372,"user_tz":-60,"elapsed":16,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","def plot_feature_importances_cancer(model):\n","  n_features=cancer.data.shape[1]\n","  plt.barh(range(n_features),model.feature_importances_,align='center')\n","  plt.yticks(np.arange(n_features),cancer.feature_names)\n","  plt.xlabel(\"feature importance\")\n","  plt.ylabel(\"features\")\n","plot_feature_importances_cancer(tree)"],"metadata":{"id":"dgyvg-0Vhuom","executionInfo":{"status":"aborted","timestamp":1674253901372,"user_tz":-60,"elapsed":16,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["However, if a feature has a low feature_importance, it doesn’t mean that this feature\n","is uninformative. It only means that the feature was not picked by the tree, likely\n","because another feature encodes the same information."],"metadata":{"id":"pSrWVQ5Rk_rn"}},{"cell_type":"markdown","source":["parameter of decision tree: max_depth, max_leaf_nodes, or min_samples_leaf—is sufficient to prevent overfitting"],"metadata":{"id":"LvxdR2vlGoJb"}},{"cell_type":"code","source":["tree=mglearn.plots.plot_tree_not_monotone()\n","display(tree)"],"metadata":{"id":"pnLhyXyWi7BL","executionInfo":{"status":"aborted","timestamp":1674253901372,"user_tz":-60,"elapsed":16,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","ram_prices = pd.read_csv(\"https://www.kaggle.com/code/shubhankartiwari/decision-tree-regression?scriptVersionId=42755779&cellId=3\")\n","plt.semilogy(ram_prices.date, ram_prices.price)\n","plt.xlabel(\"Year\")\n","plt.ylabel(\"Price in $/Mbyte\")"],"metadata":{"id":"Vb3Lw6qYn48x","executionInfo":{"status":"aborted","timestamp":1674253901373,"user_tz":-60,"elapsed":17,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Strengths weaknesses and parameters"],"metadata":{"id":"zoPzz5VxtD6k"}},{"cell_type":"markdown","source":["-The main parameter of linear models is the regularization parameter, called alpha in\n","the regression models and C in LinearSVC and LogisticRegression. Large values for\n","alpha or small values for C mean simple models. In particular for the regression mod‐\n","els, tuning these parameters is quite important. Usually C and alpha are searched for\n","on a logarithmic scale. The other decision you have to make is whether you want to\n","use L1 regularization or L2 regularization. If you assume that only a few of your fea‐\n","tures are actually important, you should use L1. Otherwise, you should default to L2.\n","L1 can also be useful if interpretability of the model is important. As L1 will use only\n","a few features, it is easier to explain which features are important to the model, and\n","what the effects of these features are.\n","\n","-Linear models are very fast to train, and also fast to predict. They scale to very large\n","datasets and work well with sparse data. If your data consists of hundreds of thou‐\n","sands or millions of samples, you might want to investigate using the solver='sag'\n","option in LogisticRegression and Ridge, which can be faster than the default on\n","large datasets. Other options are the SGDClassifier class and the SGDRegressor\n","class, which implement even more scalable versions of the linear models described\n","here"],"metadata":{"id":"gUp9psh3t0-n"}},{"cell_type":"markdown","source":["###Naive Bayes Classifiers\n","av: family of classifiers that are quite similar to the linear models.\n","it's so efficient is that they learn parameters by looking at each feature individually and collect simple per-class statistics from feature.\n","des: provide efficiency models but the generalization performance is slightly worse than the linear classifiers (logisticRegression and linearSVC)\n","there are three kinds of naive bayes classifiers implimented in sklearn : GaussianNB,BernoulliNB,MultinomiaLNB.\n"],"metadata":{"id":"v66gMaOLv4D0"}},{"cell_type":"code","source":["counts = {}\n","for label in np.unique(y):\n"," # iterate over each class\n"," # count (sum) entries of 1 per feature\n"," counts[label] = X[y == label].sum(axis=0)\n","print(\"Feature counts:\\n{}\".format(counts))"],"metadata":{"id":"2JHl7p5Hv3vb","executionInfo":{"status":"aborted","timestamp":1674253901373,"user_tz":-60,"elapsed":17,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["MultinomialNB and BernoulliNB have a single parameter, alpha, which controls\n","model complexity.This results in a\n","“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\n","complex models.\n","\n","GaussianNB is mostly used on very high-dimensional data, while the other two var‐\n","iants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\n","usually performs better than BinaryNB, particularly on datasets with a relatively large\n","number of nonzero features"],"metadata":{"id":"jXsdVRAd4FWB"}},{"cell_type":"markdown","source":["The naive Bayes models share many of the strengths and weaknesses of the linear\n","models. They are very fast to train and to predict, and the training procedure is easy\n","to understand. The models work very well with high-dimensional sparse data and are\n","relatively robust to the parameters. Naive Bayes models are great baseline models and\n","are often used on very large datasets, where training even a linear model might take\n","too long"],"metadata":{"id":"865bTPZ64qo_"}},{"cell_type":"markdown","source":["##Ensembles of Decision Trees"],"metadata":{"id":"GUdCQGKcvaVb"}},{"cell_type":"markdown","source":["Ensembles are methods that combine multiple machine learning models to create\n","more powerful models. There are many models in the machine learning literature\n","that belong to this category, but there are two ensemble models that have proven to\n","be effective on a wide range of datasets for classification and regression, both of\n","which use decision trees as their building blocks: random forests and gradient boos‐\n","ted decision trees."],"metadata":{"id":"Fm-LJZ86vgm1"}},{"cell_type":"markdown","source":["the problem of decision trees is tend to overfit the training data the random forest are one only way to address this problem "],"metadata":{"id":"zOh4O9QLwNmu"}},{"cell_type":"markdown","source":["To implement this strategy, we need to build many decision trees. Each tree should do\n","an acceptable job of predicting the target, and should also be different from the other\n","trees. Random forests get their name from injecting randomness into the tree build‐\n","ing to ensure each tree is different. There are two ways in which the trees in a random\n","forest are randomized: by selecting the data points used to build a tree and by select‐\n","ing the features in each split test. Let’s go into this process in more detail."],"metadata":{"id":"fpf-VC1Sy8ZE"}},{"cell_type":"markdown","source":["###Building random forests"],"metadata":{"id":"MKe8Rub71_Dw"}},{"cell_type":"markdown","source":["1. decide on the number of trees to builds and they will be completely independetly from each other and the algorithm will make differ‐\n","ent random choices for each tree to make sure the trees are distinct\n","2. Built a tree : call the bootstrap sample of our data ne5dhou partie mn data de5la b3adhha w tnajm tkoun feha repetition \n","3. the decision tree is build based on this newly created dataset and select a subset of the features (the number controlled by ma_features)\n","4. a high max_fea\n","tures means that the trees in the random forest will be quite similar, and they will be\n","able to fit the data easily, using the most distinctive features. A low max_features means that the trees in the random forest will be quite different, and that each tree\n","might need to be very deep in order to fit the data well.\n","5. To make a prediction using the random forest, the algorithm first makes a prediction\n","for every tree in the forest. For regression, we can average these results to get our final\n","prediction. For classification, a “soft voting” strategy is used. This means each algorithm makes a “soft” prediction, providing a probability for each possible output"],"metadata":{"id":"XXKnqA-Z2DTW"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import make_moons\n","X,y=make_moons(n_samples=100,noise=0.25,random_state=3)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n"," random_state=42)\n","forest=RandomForestClassifier(n_estimators=5,random_state=2)\n","forest.fit(X_train,y_train)"],"metadata":{"id":"NxxIzOF7ia6x","executionInfo":{"status":"aborted","timestamp":1674253901374,"user_tz":-60,"elapsed":18,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,axes=plt.subplots(2,3,figsize=(10,5))\n","for i,(ax,tree)in enumerate(zip(axes.ravel(),forest.estimators_)):\n","  ax.set_title(\"Tree{}\".format(i))\n","  mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\n","mglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],\n","  alpha=.4)\n","axes[-1, -1].set_title(\"Random Forest\")\n","mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n","  "],"metadata":{"id":"GosOqhHt6u7Y","executionInfo":{"status":"aborted","timestamp":1674253901374,"user_tz":-60,"elapsed":21160,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import load_breast_cancer\n","from sklearn.ensemble import RandomForestClassifier\n","X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n","forest.fit(X_train,y_train)\n","print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))"],"metadata":{"id":"cN6Al3GM9r-s","executionInfo":{"status":"aborted","timestamp":1674253901374,"user_tz":-60,"elapsed":21158,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_feature_importances_cancer(forest)"],"metadata":{"id":"GqbK7isSCs4-","executionInfo":{"status":"aborted","timestamp":1674253901375,"user_tz":-60,"elapsed":21158,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Similarly to the decision tree, the random forest provides feature importances, which\n","are computed by aggregating the feature importances over the trees in the forest. Typ‐\n","ically, the feature importances provided by the random forest are more reliable than\n","the ones provided by a single tree"],"metadata":{"id":"WWuH8ECLEjBN"}},{"cell_type":"markdown","source":["- The more trees there are in the forest, the more robust\n","it will be against the choice of random state. (n_estimator, max_features,If you want to have reproducible results,. Aver‐\n","aging more trees will yield a more robust ensemble by reducing overfitting.\n","it is important to fix the random_state\n","- Random forests don’t tend to perform well on very high dimensional, sparse data,\n","such as text data\n","- random\n","forests require more memory and are slower to train and to predict than linear mod‐\n","els. If time and memory are important in an application, it might make sense to use a\n","linear model instead.\n","- As described earlier, max_features determines how random each tree is, and a\n","smaller max_features reduces overfitting. In general, it’s a good rule of thumb to use\n","the default values: max_features=sqrt(n_features) for classification and max_fea\n","tures=log2(n_features) for regression. Adding max_features or max_leaf_nodes\n","might sometimes improve performance. It can also drastically reduce space and time\n","requirements for training and prediction."],"metadata":{"id":"grsmZap9FfZb"}},{"cell_type":"markdown","source":["##Gradient boosted regression trees (gradient boosting machines)\n"],"metadata":{"id":"N8GxjvHsLYR5"}},{"cell_type":"markdown","source":["- it's a mothod that combines multiples decision trees to create a more powerful model.it can be used on regression and classification \n","- it works by building trees in serial manner where each tree try to correct the mistake of the previous one. \n","- there is no randomization instead strong pre-pruning is used \n","- Each tree can only provide good\n","predictions on part of the data, and so more and more trees are added to iteratively\n","improve performance.\n","- important parameter called the learning rate, it control how strongly each tree tries to correct the mistakes of the previous trees"],"metadata":{"id":"VAlSxOE6A8y_"}},{"cell_type":"code","source":["from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.datasets import load_breast_cancer\n","cancer=load_breast_cancer()\n","X_train,X_test,y_train,y_test=train_test_split(cancer.data,cancer.target,random_state=0)\n","gbrt=GradientBoostingClassifier(random_state=0)\n","gbrt.fit(X_train,y_train)\n","print(gbrt.score(X_train,y_train))\n","print(gbrt.score(X_test,y_test))"],"metadata":{"id":"_5fqX0KnEE9l","executionInfo":{"status":"aborted","timestamp":1674253901375,"user_tz":-60,"elapsed":21157,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We are likely to overfit. we can apply stronger pre-pruning by limiting the maximum depth or lower the learning rate"],"metadata":{"id":"ITEsjObNEeIA"}},{"cell_type":"code","source":["gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n","gbrt.fit(X_train, y_train)\n","print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"],"metadata":{"id":"YeB-qQnbEJKo","executionInfo":{"status":"aborted","timestamp":1674253901375,"user_tz":-60,"elapsed":21156,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n","gbrt.fit(X_train, y_train)\n","print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))"],"metadata":{"id":"52TYbHxQFcBV","executionInfo":{"status":"aborted","timestamp":1674253901375,"user_tz":-60,"elapsed":21155,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Both methods of decreasing the model complexity reduced the training set accuracy,\n","as expected. In this case, lowering the maximum depth of the trees provided a significant improvement of the model, while lowering the learning rate only increased the\n","generalization performance slightly.\n","As for the other decision tree–based models, we can again visualize the feature\n","importances to get more insight into our model. As we used 100 trees, it\n","is impractical to inspect them all, even if they are all of depth 1:"],"metadata":{"id":"8xuqPdcUGpB7"}},{"cell_type":"code","source":["gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n","gbrt.fit(X_train, y_train)\n","plot_feature_importances_cancer(gbrt)"],"metadata":{"id":"fIdSiCVeFhib","executionInfo":{"status":"aborted","timestamp":1674253901375,"user_tz":-60,"elapsed":21154,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- the gradient boosting ignore some of the features than random forest.\n","- a common approach is to first try random forests, which work quite robustly.\n","- if we want more accuracy from the ML model, moving to gradient boosting often helps"],"metadata":{"id":"LFU3Itf9Il6C"}},{"cell_type":"markdown","source":["###Strengths, weaknesses, and parameters\n","- Gradient boosted decision trees are among the\n","most powerful and widely used models for supervised learning\n","- it require careful tuning of the parameters and may take a long time to train\n","- Similarly to other tree-based models, the algorithm works well without scaling\n","and on a mixture of binary and continuous features\n","- it often does not work well on high-dimensional sparse data\n","- PARAMETERS: n_estimators and learning rate are interconnected \n","- a lower learning rate means that moe trees are needed to build a model\n","- in contrast to random forests increasing n_estimator in GB leading to more complex model"],"metadata":{"id":"Offh6Y63K5Nl"}},{"cell_type":"markdown","source":["## Kernelized Support Vactor *Machine*"],"metadata":{"id":"CsRgrBUKOLik"}},{"cell_type":"markdown","source":["https://www.youtube.com/watch?v=Q7vT0--5VII"],"metadata":{"id":"bV-Q0HHJVwQM"}},{"cell_type":"markdown","source":["- prob: the classification that we learn previously use a linear feature and end up with a simple desicion boundery( line , plan , hyoerplan) can't be applied in real world data because most of the data are nonlinear \n","so we can't solve a nonlinear feature with a linear model \n","- solution is kernals: One way to make a linear\n","model more flexible is by adding more features—for example, by adding interactions\n","or polynomials of the input features."],"metadata":{"id":"U2nZ_H0DXEeD"}},{"cell_type":"code","source":["# add the squared first feature\n","X_new = np.hstack([X, X[:, 1:] ** 2])\n","from mpl_toolkits.mplot3d import Axes3D, axes3d\n","figure = plt.figure()\n","# visualize in 3D\n","ax = Axes3D(figure, elev=-152, azim=-26)\n","# plot first all the points with y == 0, then all with y == 1\n","mask = y == 0\n","ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n"," cmap=mglearn.cm2, s=60)\n","ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n"," cmap=mglearn.cm2, s=60)\n","ax.set_xlabel(\"feature0\")\n","ax.set_ylabel(\"feature1\")\n","ax.set_zlabel(\"feature1 ** 2\")"],"metadata":{"id":"WQjVzJTEHqY3","executionInfo":{"status":"aborted","timestamp":1674253901376,"user_tz":-60,"elapsed":21154,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["linear_svm_3d = LinearSVC().fit(X_new, y)\n","coef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n","# show linear decision boundary\n","figure = plt.figure()\n","ax = Axes3D(figure, elev=-152, azim=-26)\n","xx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\n","yy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\n","XX, YY = np.meshgrid(xx, yy)\n","ZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\n","ax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\n","ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n"," cmap=mglearn.cm2, s=60)\n","ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n"," cmap=mglearn.cm2, s=60)\n","ax.set_xlabel(\"feature0\")\n","ax.set_ylabel(\"feature1\")\n","ax.set_zlabel(\"feature0 ** 2\")"],"metadata":{"id":"LrfeRlFoUHtg","executionInfo":{"status":"aborted","timestamp":1674253901376,"user_tz":-60,"elapsed":21153,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###the kernel trick\n","it get really expencive to compute all the possible new non linear features and it take times and space --> solution: the kernel trick it works by directly computing the distance (more pre‐\n","cisely, the scalar products) of the data points for the expanded feature representation,\n","without ever actually computing the expansion.\n","- there are two ways to map with SVM : \n","1) polynomial kernel : which computes all\n","possible polynomials up to a certain degree of the original features\n","2)  The Gaussian kernel is a bit harder to explain, as it corresponds to\n","an infinite-dimensional feature space."],"metadata":{"id":"FrbsGK6Gcg35"}},{"cell_type":"markdown","source":["### Understanding the SVMs\n","Typically only a subset of\n","the training points matter for defining the decision boundary: the ones that lie on the\n","border between the classes. These are called support vectors and give the support vec‐\n","tor machine its name."],"metadata":{"id":"6dJHO3Ez4YgG"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","X,y=mglearn.tools.make_handcrafted_dataset()\n","svm=SVC(kernel='rbf',C=10,gamma=0.1).fit(X,y)\n","mglearn.plots.plot_2d_separator(svm,X,eps=5)\n","mglearn.discrete_scatter(X[:,0],X[:,1],y)\n","#plot support vectors\n","sv=svm.support_vectors_\n","# class labels of support vectors are given by the sign of the dual coefficients\n","sv_labels=svm.dual_coef_.ravel()>0\n","mglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\n","plt.xlabel(\"Feature 0\")\n","plt.ylabel(\"Feature 1\")\n"],"metadata":{"id":"907lc935Z6vT","executionInfo":{"status":"aborted","timestamp":1674253901377,"user_tz":-60,"elapsed":21153,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- GAMMA : control the width of the gaussian kernel, and it determines the scale of what it means for points to be close togather.\n","- C parameter: is a regularization parameter, similar to the used in the linear models."],"metadata":{"id":"LhEmZT406_eg"}},{"cell_type":"code","source":["fig, axes = plt.subplots(3, 3, figsize=(15, 10))\n","for ax, C in zip(axes, [-1, 0, 3]):\n"," for a, gamma in zip(ax, range(-1, 2)):\n","    mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\n","axes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n"," ncol=4, loc=(.9, 1.2))"],"metadata":{"id":"hDXoldaA6GJd","executionInfo":{"status":"aborted","timestamp":1674253901377,"user_tz":-60,"elapsed":21152,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- When gamma get higher value the decision boundery focus on single points which is more complex model.\n","- When C get higher value we get misclassification point ."],"metadata":{"id":"xQlFoOU6-IAE"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.datasets import load_breast_cancer\n","X_train, X_test, y_train, y_test = train_test_split(\n"," cancer.data, cancer.target, random_state=0)\n","svc = SVC()\n","svc.fit(X_train, y_train)\n","print(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\n","print(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))"],"metadata":{"id":"c2Cbr-AO7mOF","executionInfo":{"status":"aborted","timestamp":1674253901377,"user_tz":-60,"elapsed":21151,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(X_train.min(axis=0), 'o', label=\"min\")\n","plt.plot(X_train.max(axis=0), '^', label=\"max\")\n","plt.legend(loc=4)\n","plt.xlabel(\"Feature index\")\n","plt.ylabel(\"Feature magnitude\")\n","plt.yscale(\"log\")\n"],"metadata":{"id":"GyF3lCgy9QIV","executionInfo":{"status":"aborted","timestamp":1674253901378,"user_tz":-60,"elapsed":21151,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocessing data for SVMs\n"],"metadata":{"id":"CDFQxoixANLP"}},{"cell_type":"markdown","source":["Prob: the features are not scaled and the SVM model are very sensative to te scaling of the data\n","MinMaxscaler prepocessing method is the solution"],"metadata":{"id":"FlfszHuWAbHo"}},{"cell_type":"code","source":["# compute the minimum value per feature on the training set\n","min_on_training = X_train.min(axis=0)\n","# compute the range of each feature (max - min) on the training set\n","range_on_training = (X_train - min_on_training).max(axis=0)\n","# subtract the min, and divide by range\n","# afterward, min=0 and max=1 for each feature\n","X_train_scaled = (X_train - min_on_training) / range_on_training\n","print(\"Minimum for each feature\\n{}\".format(X_train_scaled.min(axis=0)))\n","print(\"Maximum for each feature\\n {}\".format(X_train_scaled.max(axis=0)))"],"metadata":{"id":"e7haW-RR_0eb","executionInfo":{"status":"aborted","timestamp":1674253901378,"user_tz":-60,"elapsed":21145,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_test_scaled=(X_test-min_on_training)/range_on_training"],"metadata":{"id":"dBZLWF1BBlIE","executionInfo":{"status":"aborted","timestamp":1674253901378,"user_tz":-60,"elapsed":21144,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["svc = SVC()\n","svc.fit(X_train_scaled, y_train)\n","print(\"Accuracy on training set: {:.3f}\".format(\n"," svc.score(X_train_scaled, y_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"],"metadata":{"id":"xK6l4Cm4GTI9","executionInfo":{"status":"aborted","timestamp":1674253901378,"user_tz":-60,"elapsed":21143,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["svc=SVC(C=1000)\n","svc.fit(X_train_scaled,y_train)\n","print(\"Accuracy on training set: {:.3f}\".format(\n"," svc.score(X_train_scaled, y_train)))\n","print(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))"],"metadata":{"id":"BMzgRRTYGlUi","executionInfo":{"status":"aborted","timestamp":1674253901378,"user_tz":-60,"elapsed":21142,"user":{"displayName":"jandoubi soulaima","userId":"17064568233373875983"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Strenths, weaknesses and parameters\n","- KSVM are a powerful models and perform well on a varity of datasets. \n","- they work well on low and high dimensional data \n","- it don't scale very well you need to make it by yourself \n","- by working with datasets of size 100,000 or more become more challanging of runtime and memory usage\n","- Another downside of SVMs is that they require careful preprocessing of the data and\n","tuning of the parameters this is why most people use random forest and gradient boosting \n","- SVM models are hard to inspect; it\n","can be difficult to understand why a particular prediction was made, and it might be\n","tricky to explain the model to a nonexpert.\n","- Still, it might be worth trying SVMs, particularly if all of your features represent\n","measurements in similar units and they are on similar\n","scales.\n","\n"],"metadata":{"id":"taUTbu9vJRBM"}}]}